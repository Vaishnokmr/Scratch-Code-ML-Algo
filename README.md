# Scratch-Code-ML-Algo
Scratch code for a machine learning algorithm involves writing code from scratch to implement the algorithm rather than using pre-built libraries or frameworks.
## Decision Tree - 
A decision tree is a predictive modeling tool used in machine learning and data mining. It works by partitioning data into smaller subsets based 
on the values of input variables. The goal is to create a model that predicts the value of a target variable based on several input variables. The decision tree 
consists of nodes that represent the input variables and branches that represent the possible outcomes. Each branch leads to another node or a terminal node that 
contains the predicted value. The model is built using algorithms that find the optimal way to split the data at each node.

![alt text](https://static.javatpoint.com/tutorial/machine-learning/images/decision-tree-classification-algorithm.png)

## AdaBoost -
AdaBoost, short for Adaptive Boosting, is a popular machine learning algorithm used for binary classification problems. It works by combining multiple weak learners into a strong learner. 
Weak learners are simple models that perform slightly better than random guessing. AdaBoost assigns weights to training examples, and repeatedly trains weak models on subsets of the data. 
Each weak model is given a weight based on its accuracy, and the final prediction is a weighted sum of the weak models' predictions. AdaBoost is robust to overfitting and can handle noisy data.

![alt text](https://ars.els-cdn.com/content/image/3-s2.0-B9780128177365000090-f09-18-9780128177365.jpg)


## GradientBoosting -
Gradient Boosting is a popular machine learning algorithm used for supervised learning problems. It is a type of ensemble method that combines multiple decision trees to create a stronger model.
The algorithm builds trees sequentially, with each new tree fitting the residuals of the previous tree. The residuals are the differences between the actual values and the predicted values of the previous tree. 
Gradient boosting uses gradient descent to find the optimal values of the model parameters. It is effective for both regression and classification problems and can handle large datasets.

<img src="https://habrastorage.org/web/d28/78f/7ba/d2878f7bad0340fc8002e5ba6d0879a5.jpg" width="600" height="500">

